%----------------------------------------------------------------------------
\chapter{Dimension reduction algorithms}\label{ch:dimension-reduction-algorithms}
%----------------------------------------------------------------------------




\section{PCA}\label{sec:pca}

\textit{Principal Component Analysis}\cite{bib:pca}, or PCA for short is the most widespread dimension reduction algorithm. It works by finding \textit{principal components}. These are a series of $p$ unit vectors, where the $i$-th vector minimizes the average squared distance from the points to the line, while also being orthogonal to the first $i - 1$ vectors.

PCA is the process of computing the principal components of the data and performing a change of basis, usually using only the first few principal components and ignoring the rest. This leads to a lower dimensional representation of the data, where only the components that preserve as much of the data's variation as possible.

\subsection{Mathematical details}\label{subsec:mathematical-details}

The goal of the PCA algorithm is to find unit basis vectors $\mathbf{w}_{(i)}$, such that the projected data inherits the maximum possible variance of the original data matrix $\mathbf{X}$.

The first vector $\mathbf{w}_{(1)}$ has to satisfy

\begin{equation}
	\mathbf{w}_{(1)} = \arg\max_{\lVert \mathbf{w} \rVert = 1}{ \Bigg\{ \sum_i{ \big(\mathbf{x}_{(i)} \cdot \mathbf{w} \big)^2 } \Bigg\} }
	\label{eq:pca:w1}
\end{equation}

The $k$-th component can be found by subtracting the first $k - 1$ components from $\mathbf{X}$

\begin{equation}
	\mathbf{\hat {X}}_{k}=\mathbf{X}-\sum_{s=1}^{k-1}\mathbf{X}\mathbf{w}_{(s)}\mathbf{w}_{(s)}^{\mathsf{T}}
	\label{eq:pca_xhat}
\end{equation}

and then computing the first principal component of the new matrix:

\begin{equation}
	\mathbf{w}_{(k)}=\arg\max_{\lVert \mathbf{w} \rVert = 1}{\Bigg\{ \lVert \mathbf{\hat {X}}_{k}\mathbf{w} \rVert ^2 \Bigg\} }
	\label{eq:pca:wk}
\end{equation}

The projected dataset is given by the following linear transformation:

\begin{equation}
	\mathbf{T} = \mathbf{XW}
	\label{eq:pca:t}
\end{equation}

\subsection{Benefits of PCA}\label{subsec:benefits-of-pca}

The popularity of PCA lies in its ease of use. It is easily accessible for data science use-cases, with a variety of implementations in a number of environments.

The application of PCA on a given dataset is trivial. The only parameter of the algorithm is the output dimension, and the result is a deterministic function of the dataset.\footnote{In practice this isn't entirely true, since the search for $\mathbf{w}_{(i)}$ given by equation \eqref{eq:pca:w1} cannot be implemented without some error.} This means that using PCA does not involve fine-tuning hyper-parameters, which can take a long time and takes away the focus from understanding the underlying data.

Furthermore, the PCA algorithm is simple, without any computationally expensive steps. This results in an algorithm that uses minimal hardware resources. Even working with large datasets, no supercomputer is needed for the application of principal component analysis.

\subsection{Limitations of PCA}\label{subsec:limitations-of-pca}


PCA has a number of limitations that make it unfavourable to us in certain use-cases. These limitations arise from certain assumptions made in its derivation.

One such assumption is the standardization of the used dataset. This means that the scaling of variables affect the result of PCA. Differently scaled features not only change the outcome of the transformation, but there is also a high likelihood of information loss. This can be counteracted by scaling the features of the dataset by its standard deviation, however, this is not always desirable.

Another assumption made is that the dataset contains only linear dependencies. PCA can capture linear correlations between the features but cannot capture nonlinear dependencies between them. In some cases, a transformation of the coordinates, such that the resulting representation only contains linear correlations between features, may enable PCA to successfully capture such dependencies. This is the basis for a generalization of the technique called \textit{nonlinear PCA}\cite{bib:nonlinpca}. This however requires a function that transforms the initial dataset to an entirely linearly dependent representation, which can be difficult to find, and in some cases is impossible.

\section{t-SNE}\label{sec:t-sne}

\textit{t-distributed stochastic neighbour embedding}\cite{bib:tsne}, abbreviated to t-SNE is one of the more widely methods for visualizing high dimensional data in usually two or three dimensions. The technique is based on stochastic neighbour embedding\cite{bib:sne} but utilising Student's t-distribution.

\subsection{Algorithm}

t-SNE consists of two main stages. The first one is constructing a probability distribution over pairs of high dimensional objects such that similar points are assigned higher probabilities while dissimilar objects are assigned lower probability. In the second stage, t-SNE constructs a similar probability distribution over the points in the low-dimensional map, then minimises the  Kullbackâ€“Leibler divergence\cite{bib:kldiv} between the two distributions with respect to the positions of points in the map. The original version of t-SNE uses Euclidean distance as a metric for similarity, however, this can be changed in popular implementations where a different metric is more appropriate.

In the first step of the algorithm, a probability distribution is constructed over the pairs of high dimensional objects. The exact formula for the probability assigned to the pair $\mathbf{x}_i$ and $\mathbf{x}_j$ is given by

\begin{equation}
	\label{eq:tsne:pi|j}
	p_{i|j} =
	\begin{cases}
		{\frac {\exp(-\lVert \mathbf {x} _{i}-\mathbf {x} _{j}\rVert ^{2}/2\sigma _{i}^{2})}{\sum _{k\neq i}\exp(-\lVert \mathbf {x} _{i}-\mathbf {x} _{k}\rVert ^{2}/2\sigma _{i}^{2})}} & \textrm{if } i \neq j \\
		\hfil 0 & \textrm{if } i = j
	\end{cases}
\end{equation}

Van der Maaten and Hilton in their paper\cite{bib:tsne} give a very good explanation for this probability. ,,The similarity of data point $\mathbf{x}_j$ to data point $\mathbf{x}_i$ is the conditional probability, $p_{j|i}$, that $\mathbf{x}_i$ would pick $\mathbf{x}_j$ as its neighbour if neighbours were picked in proportion to their probability density under a Gaussian centred at $\mathbf{x}_i$.''

Since this formula depends on the order of points ($p_{i|j} \neq p_{j|i}$), we define

\begin{equation}
	\label{eq:tsne:pij}
	p_{ij}={\frac {p_{j\mid i}+p_{i\mid j}}{2N}}
\end{equation}

where $N$ is the number of data points. Note that $p_{ij} = p_{ji}$, $p_{ii} = 0$ and $\displaystyle\sum_{i, j}p_{ij} = 1$.

The value of $\sigma_{i}$ is set in such a way that the \textit{perplexity} of the conditional distribution equals the predefined perplexity value given to t-SNE as a hyperparameter. Specifically, SNE performs a binary search for the value of $\sigma_{i}$ that produces probability distribution with a fixed perplexity that is specified by the user. This perplexity is calculated by the following formula:

\begin{equation}
	\label{eq:tsne:perlexity}
	Perp(P_i) = 2^{-\sum_{p_{j|i}} \log_2 p_{j|i}}
\end{equation}

For the lower dimensional map, a similar probability distribution is constructed over pairs of low dimensional point pairs.

\begin{equation}
	\label{eq:tsne:qij}
	q_{ij} =
	\begin{cases}
		{\frac{(1+\lVert \mathbf{y}_{i} - \mathbf{y}_{j}\rVert^{2})^{-1}}{\sum_{k}\sum_{l \neq k}(1+\lVert \mathbf{y}_{k} - \mathbf{y}_{l}\rVert ^{2})^{-1}}} & \textrm{if } i \neq j \\
		\hfil 0 & \textrm{if } i = j
	\end{cases}
\end{equation}

Herein a heavy-tailed \textit{Student's t-distribution} is used to measure the similarity between low-dimensional points in order to allow dissimilar objects to be modelled far apart in the map.

The \textit{Kullback-Leibler divergence} (KL-divergence)\cite{bib:kldiv} of the probability distributions $P$ and $Q$ is then calculated as an error function:

\begin{equation}
	\label{eq:KLdiv}
 	\mathrm{KL} \left(P \parallel Q \right) = \sum_{i\neq j} p_{ij} \log{\frac{p_{ij}}{q_{ij}}}
\end{equation}

The final locations of the points $\mathrm{y}_i$ in the map are determined by minimising the Kullback-Leibler divergence of the probability distributions with respect to the points $\mathrm{y}_i$. This minimization is performed by gradient descent. The result of this optimization is a map that reflects the similarities between the high-dimensional inputs.

\subsection{Advantages over PCA}

t-SNE is a nonlinear dimensionality reduction algorithm, and as such can be used on datasets with nonlinear dependencies -- relationships in the data that PCA cannot capture. This property is essential in applications in computational chemistry, \textit{de novo} molecule generation and bioinformatics, where gathered data is rarely linear in nature.

Using t-SNE does not require standardization of data, as the used metric is the distance of data points. This however is not an advantage in cases where the data used is already standardized.

The algorithm has many hyperparameters, such as perplexity, and all parameters of gradient descent, such as learning rate, early exaggeration and number of iterations. This means that data scientists can ,,tinker'' with t-SNE to find an embedding suitable for their use-case.

\subsection{Disadvantages}



\section{UMAP}\label{sec:umap}



\section{TriMAP}\label{sec:trimap}



\section{PaCMAP}\label{sec:pacmap}


